{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"$CONDA_PREFIX/lib/python3.9/site-packages/nvidia/cudnn/lib:$LD_LIBRARY_PATH\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 14:07:08.866806: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-26 14:07:08.891629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-26 14:07:09.243904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 14:07:11.912757: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-26 14:07:11.930133: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-26 14:07:11.930231: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(dataset_dir, subject_n=5, img_size=(8, 9, 8), number_of_inputs=1,\n",
    "              features_type='multi', num_classes=2, frames_per_subject=4800, seed=7):\n",
    "    img_rows, img_cols, num_chan = img_size\n",
    "\n",
    "    prefixs = ['DE/DE_s', 'PSD/PSD_s']\n",
    "\n",
    "    if features_type == 'DE':\n",
    "        prefixs = prefixs[:1]\n",
    "    elif features_type == 'PSD':\n",
    "        prefixs = prefixs[1:]\n",
    "    elif features_type != 'multi':\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    samples_number_per_subject = int(frames_per_subject / number_of_inputs)  # tested only for [1...6] range\n",
    "    samples_numbers_list = list(range(samples_number_per_subject))\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    y_v_list = []\n",
    "\n",
    "    x_list = []\n",
    "\n",
    "    subject_id_list = []\n",
    "\n",
    "    for i in range(subject_n):\n",
    "        short_name = f'{i + 1:02}'\n",
    "        random.shuffle(samples_numbers_list)\n",
    "        print(\"\\nprocessing: \", short_name, \"......\")\n",
    "        file_path = os.path.join(dataset_dir, prefixs[0] + short_name)\n",
    "        file = sio.loadmat(file_path)\n",
    "        data = file['data']\n",
    "        y_v = file['valence_labels'][0]\n",
    "        test = 0\n",
    "        for i in y_v:\n",
    "            test += i\n",
    "        print(test)\n",
    "        print(\"y_v[0:10]: \",y_v[0:10])\n",
    "        if len(prefixs) > 0:\n",
    "            for prefix in prefixs[1:]:\n",
    "                file_path = os.path.join(dataset_dir, prefix + short_name)\n",
    "                file = sio.loadmat(file_path)\n",
    "                data = np.concatenate([data, file['data']], axis=1)\n",
    "\n",
    "        print(\"Data shape: \", data.shape)\n",
    "        print(\"Labels shape: \", y_v.shape)\n",
    "        one_falx = data.transpose([0, 2, 3, 1])\n",
    "        one_falx = one_falx.reshape((-1, number_of_inputs, img_rows, img_cols, num_chan))\n",
    "        one_y_v = np.empty([0, 1])\n",
    "\n",
    "        print(\"one_falx shape: \", one_falx.shape)\n",
    "        print(\"one_y_v shape: \", one_y_v.shape)\n",
    "    \n",
    "        for j in range(int(len(y_v) // number_of_inputs)):\n",
    "            one_y_v = np.vstack([one_y_v, y_v[j * number_of_inputs]])\n",
    "\n",
    "        one_falx = one_falx[samples_numbers_list]\n",
    "        one_y_v = one_y_v[samples_numbers_list]\n",
    "        print(\"one_falx shape: \", one_falx.shape)\n",
    "        print(\"one_y_v shape: \", one_y_v.shape)\n",
    "\n",
    "        subject_id = np.array([i] * samples_number_per_subject)\n",
    "        print(\"subject_id shape: \", subject_id.shape)\n",
    "\n",
    "        y_v_list.append(one_y_v)\n",
    "\n",
    "        x_list.append(one_falx)\n",
    "\n",
    "\n",
    "\n",
    "        subject_id_list.append(subject_id)\n",
    "\n",
    "    y_v_all_subject = np.concatenate(y_v_list)\n",
    "\n",
    "    x_all_subject = np.concatenate(x_list)\n",
    "    all_subject_id = np.concatenate(subject_id_list)\n",
    "\n",
    "    print(\"y_v_all_subject shape: \", y_v_all_subject.shape)\n",
    "    print(\"x_all_subject shape: \", x_all_subject.shape)\n",
    "    print(\"all_subject_id shape: \", all_subject_id.shape)\n",
    "\n",
    "    x_all_subject_train, x_all_subject_test, y_v_all_subject_train, y_v_all_subject_test = train_test_split( x_all_subject, y_v_all_subject, test_size=0.2, random_state=42)\n",
    "    return y_v_all_subject_train, y_v_all_subject_test, x_all_subject_train, x_all_subject_test, all_subject_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 16:33:37.481367: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-02 16:33:37.723877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-02 16:33:38.242756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import (\n",
    "    Average,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    MaxPooling2D,\n",
    "    Reshape,\n",
    ")\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "\n",
    "def create_base_network(input_dim, dropout_rate):\n",
    "    seq = Sequential()\n",
    "    seq.add(keras.Input(input_dim))\n",
    "    seq.add(\n",
    "        Conv2D(\n",
    "            64,\n",
    "            5,\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "            name=\"conv1\",\n",
    "        )\n",
    "    )\n",
    "    if True:\n",
    "        seq.add(BatchNormalization())\n",
    "    seq.add(Dropout(dropout_rate))\n",
    "    seq.add(Conv2D(128, 4, activation=\"relu\", padding=\"same\", name=\"conv2\"))\n",
    "    if True:\n",
    "        seq.add(BatchNormalization())\n",
    "    seq.add(Dropout(dropout_rate))\n",
    "    seq.add(Conv2D(256, 4, activation=\"relu\", padding=\"same\", name=\"conv3\"))\n",
    "    if True:\n",
    "        seq.add(BatchNormalization())\n",
    "    seq.add(Dropout(dropout_rate))\n",
    "    seq.add(Conv2D(64, 1, activation=\"relu\", padding=\"same\", name=\"conv4\"))\n",
    "    seq.add(MaxPooling2D(2, 2, name=\"pool1\"))\n",
    "    if True:\n",
    "        seq.add(BatchNormalization())\n",
    "    seq.add(Dropout(dropout_rate))\n",
    "    seq.add(Flatten(name=\"fla1\"))\n",
    "    seq.add(Dense(512, activation=\"relu\", name=\"dense1\"))\n",
    "    seq.add(Reshape((1, 512), name=\"reshape\"))\n",
    "    if True:\n",
    "        seq.add(BatchNormalization())\n",
    "    seq.add(Dropout(dropout_rate))\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "def create_MT_CNN(img_size=(8, 9, 8), dropout_rate=0.2, number_of_inputs=1):\n",
    "\n",
    "    base_network = create_base_network(img_size, dropout_rate)\n",
    "\n",
    "    inputs = [Input(shape=img_size) for i in range(number_of_inputs)]\n",
    "\n",
    "    if number_of_inputs == 1:\n",
    "        x = base_network(inputs[0])\n",
    "    else:\n",
    "        x = Average()([base_network(input_) for input_ in inputs])\n",
    "\n",
    "    x = Flatten(name=\"flat\")(x)\n",
    "\n",
    "    # out_v = Dense(2, activation=\"softmax\", name=\"out_v\")(x)\n",
    "    # out_a = Dense(2, activation=\"softmax\", name=\"out_a\")(x)\n",
    "    out_v = Dense(1, activation=\"sigmoid\", name=\"out_v\")(x)\n",
    "\n",
    "    # model = Model(inputs, [out_v, out_a])\n",
    "    model = Model(inputs, out_v)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_sample_weights(list_of_y_trains):\n",
    "    print(\"[get_sample_weights] list_of_y_trains: \",list_of_y_trains)\n",
    "    sample_weights = np.ones(list_of_y_trains[0].shape[0])\n",
    "    print(\"[get_sample_weights] sample_weights shape: \", sample_weights.shape)\n",
    "\n",
    "    for y_train in list_of_y_trains:\n",
    "        print(\"[get_sample_weights] y_train shape: \", y_train.shape)\n",
    "        y_ints = y_train.argmax(1)\n",
    "        print(\"[get_sample_weights] y_ints shape: \", y_ints.shape)\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                          classes=np.unique(y_ints),\n",
    "                                                          y=y_ints)\n",
    "        print(\"[get_sample_weights] class_weights shape: \", class_weights.shape)\n",
    "        print(class_weights)\n",
    "    for i in np.unique(y_ints):\n",
    "        sample_weights[y_ints == i] = \\\n",
    "            sample_weights[y_ints == i] * class_weights[i]\n",
    "        print(sample_weights)\n",
    "\n",
    "    return sample_weights\n",
    "\n",
    "def print_results(scores_dict, fine_tuning):\n",
    "    for scores_name, scores_path in scores_dict.items():\n",
    "        with open(scores_path, \"rb\") as fl:\n",
    "            scores = pickle.load(fl)\n",
    "\n",
    "        accuracy_score = np.mean([score[-1] for score in scores]) * 100\n",
    "        loss_score = np.mean([score[0] for score in scores]) * 100\n",
    "\n",
    "        print(f\"\\n{scores_name}\")\n",
    "        print(f\"Accuracy mean: {accuracy_score:3.4f} %\")\n",
    "        print(f\"Loss mean: {loss_score:3.4f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from shutil import copyfile\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def train(\n",
    "    x_all_subject_train,\n",
    "    x_all_subject_test,\n",
    "    y_v_all_subject_train,\n",
    "    y_v_all_subject_test,\n",
    "    dropout_rate=0.2,\n",
    "    number_of_inputs=1,\n",
    "    model_dir=\".\",\n",
    "    metrics_dir=\".\",\n",
    "    model_name=\"MT_CNN\",\n",
    "    img_size=(8, 9, 8),\n",
    "    epochs_n=200,\n",
    "    seed=7,\n",
    "    verbose=0,\n",
    "):\n",
    "\n",
    "    lrate = lambda model_checkpoint_path: ReduceLROnPlateau(\n",
    "        best_path=model_checkpoint_path,\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        factor=0.5,\n",
    "        verbose=1,\n",
    "        \n",
    "    )\n",
    "\n",
    "    es = lambda: EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=16)\n",
    "\n",
    "    print(\"Save freq: \", int(y_v_all_subject_train.shape[0]/64*5))\n",
    "    save_model = lambda model_checkpoint_path: ModelCheckpoint(\n",
    "        model_checkpoint_path,\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode=\"min\",\n",
    "        save_freq=7680,\n",
    "        # period=5\n",
    "    )\n",
    "\n",
    "    scores_subject_independent_list = []\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    n_splits = 5\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for fold, (train, val) in enumerate(\n",
    "        kfold.split(x_all_subject_train, y_v_all_subject_train)\n",
    "    ):  \n",
    "        print(\"Fold: \", fold)\n",
    "        print(\"Val: \", val)\n",
    "        print(\"Training: \", train)\n",
    "        print(\"Train size: \", len(train))\n",
    "        print(f\"\\n\\nFold {fold + 1}/{n_splits}\\n\\n\")\n",
    "        # if fold > 1:\n",
    "        #    continue\n",
    "        K.clear_session()\n",
    "\n",
    "        model_checkpoint_path_SI_unique = (\n",
    "            f\"{model_dir}/{model_name}-weight_AV-fold{fold + 1:02d}\"\n",
    "            + \"-epoch{epoch:02d}-loss{loss:.2f}-V_accuracy{accuracy:.4f}.keras\"\n",
    "        )\n",
    "        model_checkpoint_path_SI_for_load = (\n",
    "            f\"{model_dir}/{model_name}-weight_AV-fold{fold + 1:02d}.keras\"\n",
    "        )\n",
    "\n",
    "        model = create_MT_CNN(img_size, dropout_rate, number_of_inputs)\n",
    "\n",
    "        model.compile(\n",
    "            loss=keras.losses.BinaryCrossentropy(),\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            metrics={'out_v':'accuracy'},\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        x_train = x_all_subject_train[train]\n",
    "        y_train_v = y_v_all_subject_train[train]\n",
    "\n",
    "\n",
    "        x_val = x_all_subject_train[val]\n",
    "        y_val_v = y_v_all_subject_train[val]\n",
    "\n",
    "        x_test = x_all_subject_test\n",
    "        y_test_v = y_v_all_subject_test\n",
    "        \n",
    "        print(\"Type x_train: \", type(x_train))\n",
    "        print(\"Type y_train: \", type(y_train_v))\n",
    "        print(\"Shape x_train: \", x_train.shape)\n",
    "        print(\"Shape y_train: \", y_train_v.shape)\n",
    "\n",
    "        print(\"Type [x_train[:, i] for i in range(x_train.shape[1])]: \", type([x_train[:, i] for i in range(x_train.shape[1])]))\n",
    "        print(\"Type [y_train_v]: \", type([y_train_v]))\n",
    "        \n",
    "        hist = model.fit(\n",
    "            [x_train[:, i] for i in range(x_train.shape[1])],\n",
    "            y_train_v,\n",
    "            epochs=epochs_n,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            callbacks=[\n",
    "                save_model(model_checkpoint_path_SI_unique),\n",
    "                save_model(model_checkpoint_path_SI_for_load),\n",
    "                lrate(model_checkpoint_path_SI_for_load),\n",
    "                es(),\n",
    "            ],\n",
    "            validation_data=(\n",
    "                [x_val[:, i] for i in range(x_val.shape[1])],\n",
    "                y_val_v,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        scores = model.evaluate(\n",
    "            [x_test[:, i] for i in range(x_test.shape[1])],\n",
    "            y_test_v,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        scores_subject_independent_list.append(scores)\n",
    "\n",
    "        with open(os.path.join(metrics_dir, f\"{model_name}_scores_SI.pkl\"), \"wb\") as fl:\n",
    "            pickle.dump(scores_subject_independent_list, fl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing:  01 ......\n",
      "2280.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  02 ......\n",
      "2640.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  03 ......\n",
      "2640.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  04 ......\n",
      "1920.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  05 ......\n",
      "2880.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  06 ......\n",
      "3600.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  07 ......\n",
      "3360.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  08 ......\n",
      "2640.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  09 ......\n",
      "2400.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  10 ......\n",
      "2400.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  11 ......\n",
      "2880.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  12 ......\n",
      "2520.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  13 ......\n",
      "2040.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  14 ......\n",
      "2400.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  15 ......\n",
      "2400.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  16 ......\n",
      "1800.0\n",
      "y_v[0:10]:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  17 ......\n",
      "2640.0\n",
      "y_v[0:10]:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  18 ......\n",
      "2880.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  19 ......\n",
      "2760.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  20 ......\n",
      "2760.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  21 ......\n",
      "2520.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  22 ......\n",
      "2160.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  23 ......\n",
      "3120.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  24 ......\n",
      "2160.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  25 ......\n",
      "2280.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  26 ......\n",
      "3120.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  27 ......\n",
      "3600.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  28 ......\n",
      "3000.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  29 ......\n",
      "2760.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  30 ......\n",
      "3240.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  31 ......\n",
      "2760.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "\n",
      "processing:  32 ......\n",
      "2400.0\n",
      "y_v[0:10]:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Data shape:  (4800, 8, 8, 9)\n",
      "Labels shape:  (4800,)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (0, 1)\n",
      "one_falx shape:  (4800, 1, 8, 9, 8)\n",
      "one_y_v shape:  (4800, 1)\n",
      "HHHHHHHHHHHHHHHHHHHHHHH\n",
      "subject_id shape:  (4800,)\n",
      "y_v_all_subject shape:  (153600, 1)\n",
      "x_all_subject shape:  (153600, 1, 8, 9, 8)\n",
      "all_subject_id shape:  (153600,)\n",
      "\n",
      "results\n",
      "Accuracy mean: 94.4824 %\n",
      "Loss mean: 14.8330 %\n"
     ]
    }
   ],
   "source": [
    "# specify dataset and model dirs\n",
    "dataset_dir = \"./preprocessed_data/3D\"  # path of the folder with PSD_s and DE_s files\n",
    "model_dir = \"./model\"  # path where the model and metrics will be stored\n",
    "metrics_dir = \"./metrics_bk\"  # path to the folder were all metrics will be stored\n",
    "\n",
    "img_size = img_rows, img_cols, num_chan = 8, 9, 8  # matrix shape of input data\n",
    "number_of_inputs = 1  # how many frames is taken into account during one pass\n",
    "\n",
    "features_type = \"multi\"  # 'PSD', 'DE' or 'multi' be carefull with num_chan\n",
    "num_classes = 2  # number of classes of input data\n",
    "frames_per_subject = 4800  # how many frames per one subject\n",
    "seed = 7  # random seed\n",
    "\n",
    "dropout_rate = 0.2\n",
    "model_name = \"MT_CNN\"  # will be a filename part\n",
    "epochs_n = 50  # maximum number of epochs\n",
    "verbose = 0  # 0, 1 or 2\n",
    "\n",
    "subject_n = 32\n",
    "\n",
    "\n",
    "y_v_all_subject_train, y_v_all_subject_test, x_all_subject_train, x_all_subject_test, all_subject_id = load_data(\n",
    "    dataset_dir,\n",
    "    subject_n,\n",
    "    img_size,\n",
    "    number_of_inputs,\n",
    "    features_type,\n",
    "    num_classes,\n",
    "    frames_per_subject,\n",
    "    seed,\n",
    ")\n",
    "\n",
    "train(\n",
    "    x_all_subject_train,\n",
    "    x_all_subject_test,\n",
    "    y_v_all_subject_train,\n",
    "    y_v_all_subject_test,\n",
    "    dropout_rate,\n",
    "    number_of_inputs,\n",
    "    model_dir,\n",
    "    metrics_dir,\n",
    "    model_name,\n",
    "    img_size,\n",
    "    epochs_n,\n",
    "    seed,\n",
    "    verbose,\n",
    ")\n",
    "\n",
    "\n",
    "scores_dict = {\n",
    "    \"results\": f\"{metrics_dir}/{model_name}_scores_SI.pkl\"\n",
    "}  # dict with path of scores file\n",
    "print_results(scores_dict, fine_tuning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model_bk/MT_CNN-weight_AV-fold05.keras\n"
     ]
    }
   ],
   "source": [
    "list_of_files = glob.glob('./model_bk/*') # * means all if need specific format then *.csv\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "print(latest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieuvd/miniconda3/envs/nmttnt/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 46 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9469 - loss: 0.1425 - precision: 0.9544 - recall: 0.9499\n",
      "['loss', 'compile_metrics']\n",
      "[0.14670242369174957, 0.9463541507720947, 0.9542313814163208, 0.948740541934967]\n"
     ]
    }
   ],
   "source": [
    "model = create_MT_CNN(img_size, dropout_rate, number_of_inputs)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics={'out_v':['accuracy', 'precision', 'recall']},\n",
    ")\n",
    "\n",
    "model.load_weights(latest_file)\n",
    "\n",
    "scores = model.evaluate(\n",
    "            [x_all_subject_test[:, i] for i in range(x_all_subject_test.shape[1])],\n",
    "            y_v_all_subject_test,\n",
    "            verbose=1,\n",
    "        )\n",
    "print(model.metrics_names)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmttnt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
